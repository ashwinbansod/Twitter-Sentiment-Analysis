{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets:7182\n",
      "top few tweets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>Anootated tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:28:53-05:00</td>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-10 00:00:00</td>\n",
       "      <td>10:09:00-05:00</td>\n",
       "      <td>Question: If &lt;e&gt;Romney&lt;/e&gt; and &lt;e&gt;Obama&lt;/e&gt; ha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:04:30-05:00</td>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:00:36-05:00</td>\n",
       "      <td>RT @davewiner Slate: Blame &lt;e&gt;Obama&lt;/e&gt; for fo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>09:50:08-05:00</td>\n",
       "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date            time  \\\n",
       "0  2012-10-16 00:00:00  10:28:53-05:00   \n",
       "1  2016-12-10 00:00:00  10:09:00-05:00   \n",
       "2  2012-10-16 00:00:00  10:04:30-05:00   \n",
       "3  2012-10-16 00:00:00  10:00:36-05:00   \n",
       "4  2012-10-16 00:00:00  09:50:08-05:00   \n",
       "\n",
       "                                     Anootated tweet Class  \n",
       "0  Kirkpatrick, who wore a baseball cap embroider...     0  \n",
       "1  Question: If <e>Romney</e> and <e>Obama</e> ha...     2  \n",
       "2  #<e>obama</e> debates that Cracker Ass Cracker...     1  \n",
       "3  RT @davewiner Slate: Blame <e>Obama</e> for fo...     2  \n",
       "4  @Hollivan @hereistheanswer  Youre missing the ...     0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_excel('training-Obama-Romney-tweets.xlsx', sheetname = 'Obama')\n",
    "inputFrame = df1\n",
    "inputFrame.dropna(inplace = True)\n",
    "print('Total number of tweets:'+str(len(inputFrame)))\n",
    "print('top few tweets:')\n",
    "inputFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing (cleaning the tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputFrame_test = inputFrame.copy()\n",
    "inputFrame_test['Class'] = inputFrame_test['Class'].astype(str) \n",
    "inputFrame1 = inputFrame_test[inputFrame_test.Class == '1']\n",
    "inputFrame2 = inputFrame_test[inputFrame_test.Class == '-1']\n",
    "inputFrame3 = inputFrame_test[inputFrame_test.Class == '0']\n",
    "inputFrame = pd.concat([inputFrame1, inputFrame2, inputFrame3])\n",
    "tweetProcessFrame = inputFrame.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleaning tweets\n",
    "\n",
    "pwords,words=None, None\n",
    "\n",
    "with open(\"positive-words.txt\") as f:\n",
    "    pwords = [el.strip() for el in f.readlines()]\n",
    "with open(\"negative-words.txt\") as f:\n",
    "    nwords = [el.strip() for el in f.readlines()]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "emoticons_dict = {}\n",
    "def populateEmoticonsDict():\n",
    "    fileHandler = open('EmoticonsWithPolarity.txt', 'r')\n",
    "    for line in fileHandler:\n",
    "        emoticon_list = line[:-1].split(' ')\n",
    "        sentiment = emoticon_list[-1]\n",
    "        emoticon_list = emoticon_list[:-1]\n",
    "        for emoticon in emoticon_list:\n",
    "            emoticons_dict[emoticon] = sentiment\n",
    "populateEmoticonsDict()\n",
    "\n",
    "def replace(tweet,words,replacement):\n",
    "    li = []\n",
    "    for word in tweet:\n",
    "        if word in words:\n",
    "            li.append(replacement)\n",
    "        else:\n",
    "            li.append(word)\n",
    "    return li\n",
    "\n",
    "def processTweet(tweet):\n",
    "    emotionFreeTweet = mapEmoticons(tweet)\n",
    "    tagFreeTweet = removeTags(emotionFreeTweet)\n",
    "    lowerCaseTweet = tagFreeTweet.lower()\n",
    "    ptweet = replace(lowerCaseTweet.split(' '),pwords,'positive')\n",
    "    ntweet = replace(ptweet,nwords,'negative')\n",
    "    t = ' '.join(ntweet)\n",
    "    puncFreeTweet = removePunctuations(t)\n",
    "    stopFreeTweet = removeStopWords(puncFreeTweet, stop_words)\n",
    "    stemmedTweet = stemWords(stopFreeTweet)\n",
    "    return stemmedTweet\n",
    "\n",
    "def stemWords(tweet):\n",
    "    return [stemmer.stem(t) for t in tweet]\n",
    "\n",
    "def mapEmoticons(tweet):\n",
    "    list_words = tweet.split(' ')\n",
    "    newtweet = \"\"\n",
    "    for word in list_words:\n",
    "        if word in emoticons_dict:\n",
    "            newtweet = newtweet + ' ' + emoticons_dict.get(word) \n",
    "        else:\n",
    "            newtweet = newtweet + ' ' + word\n",
    "    return newtweet\n",
    "    \n",
    "\n",
    "def removeTags(tweet):\n",
    "    cleanr = re.compile('(</?[a-zA-Z]+>|https?:\\/\\/[^\\s]*|(^|\\s)RT(\\s|$)|@[^\\s]+|\\d+)')\n",
    "    cleantext = re.sub(cleanr, ' weblink ', tweet)\n",
    "    cleantext = re.sub('(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)',' usermention ',cleantext)\n",
    "    cleantext = re.sub('[^\\sa-zA-Z]+','',cleantext)\n",
    "    cleantext = re.sub('\\s+',' ',cleantext)\n",
    "    return cleantext\n",
    "\n",
    "def removePunctuations(tweet):\n",
    "    exclude = set(string.punctuation) \n",
    "    t = ''\n",
    "    for ch in tweet:\n",
    "        if ch not in exclude:\n",
    "            t+=ch\n",
    "        else:\n",
    "            t+=' '\n",
    "    return tweet\n",
    "\n",
    "def removeStopWords(tweet, stop_words):\n",
    "    words_tokenize = word_tokenize(tweet)\n",
    "    filtered_sentence = words_tokenize #[w for w in words_tokenize if w not in stop_words]\n",
    "    return filtered_sentence #' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets after preprocessing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weblink obama weblink debat that cracker ass c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>still my posit mr presid weblink obama weblink</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>weblink you said so much posit thing about web...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>weblink im a south african and i say weblink o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>okay weblink obama weblink it is time to put y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet Class\n",
       "2   weblink obama weblink debat that cracker ass c...     1\n",
       "25     still my posit mr presid weblink obama weblink     1\n",
       "29  weblink you said so much posit thing about web...     1\n",
       "32  weblink im a south african and i say weblink o...     1\n",
       "44  okay weblink obama weblink it is time to put y...     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetProcessFrame.rename(columns={'date':'date','time':'time','Anootated tweet' : 'tweet','Class':'Class'},inplace = True)\n",
    "tweetProcessFrame['tweet'] = tweetProcessFrame['tweet'].apply(processTweet)\n",
    "del tweetProcessFrame['time']\n",
    "del tweetProcessFrame['date']\n",
    "def joinList1(tweetList):\n",
    "    return \" \".join(tweetList)\n",
    "\n",
    "tweetProcessFrame['tweet'] = tweetProcessFrame['tweet'].apply(joinList1)\n",
    "tweetProcessFrame1 = pd.DataFrame.drop_duplicates(tweetProcessFrame)\n",
    "print('tweets after preprocessing')\n",
    "tweetProcessFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "spliting training and testing for 80-20 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train, test = train_test_split(tweetProcessFrame, test_size = 0.2)\n",
    "def splitTrainingData(df, train_data_prcnt=80):\n",
    "    msk = np.random.rand(len(df)) < train_data_prcnt/100\n",
    "    train = df[msk]\n",
    "    test = df[~msk]\n",
    "    return train, test\n",
    "tweet_random_df = tweetProcessFrame1.copy()\n",
    "for i in range(0, 50):\n",
    "    split1_df, split2_df = splitTrainingData(tweet_random_df)\n",
    "    tweet_random_df = pd.concat([split1_df, split2_df])\n",
    "train, test = splitTrainingData(tweet_random_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = train['tweet']\n",
    "train_label = train['Class']\n",
    "train_label = pd.to_numeric(train_label)\n",
    "\n",
    "test_data = test['tweet']\n",
    "test_class = test['Class']\n",
    "test_class = pd.to_numeric(test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(max_features = 4800, ngram_range=(1, 2))\n",
    "X_train_counts = count_vect.fit_transform(train_data)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.568716094033\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.56      0.63      0.59       407\n",
      "          0       0.51      0.50      0.51       373\n",
      "          1       0.66      0.57      0.61       326\n",
      "\n",
      "avg / total       0.57      0.57      0.57      1106\n",
      "\n",
      "Confusion Matrix:\n",
      "[[255 118  34]\n",
      " [124 188  61]\n",
      " [ 79  61 186]]\n",
      "Train Accuracy:0.770477047705\n"
     ]
    }
   ],
   "source": [
    "# mnbd = {   \n",
    "#             'alpha':[(float)(el/1000) for el in range(0,100,1)], \n",
    "#             'fit_prior':(True,False)\n",
    "#         }\n",
    "\n",
    "clf = MultinomialNB(alpha= 0.99, fit_prior= True).fit(X_train_tf, train_label)\n",
    "#GridSearchCV(MultinomialNB(),mnbd).fit(X_train_tf, train_label)\n",
    "#clf.best_params_\n",
    "# clf = MultinomialNB(alpha= 0.99, fit_prior= True).fit(X_train_tf, train_label)\n",
    "X_test_counts = count_vect.transform(test_data)\n",
    "X_test_tfidf = tf_transformer.transform(X_test_counts)\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "\n",
    "print('Test Accuracy:'+str(np.mean(predicted == test_class)))\n",
    "\n",
    "print(metrics.classification_report(test_class, predicted))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "nb_confusion_matrix = metrics.confusion_matrix(test_class, predicted)\n",
    "print(nb_confusion_matrix)\n",
    "\n",
    "predicted_train = clf.predict(X_train_tf)\n",
    "print('Train Accuracy:'+str(np.mean(predicted_train == train_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (Linear SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.591320072333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.57      0.66      0.61       407\n",
      "          0       0.57      0.50      0.53       373\n",
      "          1       0.64      0.62      0.63       326\n",
      "\n",
      "avg / total       0.59      0.59      0.59      1106\n",
      "\n",
      "Confusion Matrix:\n",
      "[[268  92  47]\n",
      " [121 185  67]\n",
      " [ 78  47 201]]\n",
      "Train Accuracy:0.794329432943\n"
     ]
    }
   ],
   "source": [
    "# parameterssvm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#               'vect__max_features': list(range(500,5500,100)),\n",
    "#               'tfidf__use_idf': (True, False),\n",
    "#               'clf__loss' : ['hinge', 'squared_hinge'],\n",
    "#               'clf__penalty' : ['l2'],\n",
    "#               'clf__C' : [i/10 for i in range(5,9)],\n",
    "#               'clf__multi_class' : ['crammer_singer','ovr']\n",
    "#              }\n",
    "\n",
    "# text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "#                     ('tfidf', TfidfTransformer()),\n",
    "#                     ('clf', LinearSVC(random_state= 42, max_iter=10000))])\n",
    "# text_clf_svm = text_clf_svm.fit(train_data,train_label)\n",
    "# text_clf_svm = GridSearchCV(text_clf_svm,parameterssvm,n_jobs=-1).fit(train_data, train_label)\n",
    "# text_clf_svm.best_params_\n",
    "\n",
    "# {'clf__C': 0.5,\n",
    "#  'clf__loss': 'hinge',\n",
    "#  'clf__multi_class': 'ovr',\n",
    "#  'clf__penalty': 'l2',\n",
    "#  'tfidf__use_idf': True,\n",
    "#  'vect__max_features': 4800,\n",
    "#  'vect__ngram_range': (1, 2)}\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(max_features=4800,ngram_range=(1,2))),\n",
    "                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ('clf', LinearSVC(C=0.5,loss='hinge',multi_class='ovr',penalty='l2',random_state= 42, max_iter=10000))]).fit(train_data, train_label)\n",
    "\n",
    "predicted_svm = text_clf_svm.predict(test_data)\n",
    "print('Test Accuracy:'+str(np.mean(predicted_svm == test_class)))\n",
    "print(metrics.classification_report(test_class, predicted_svm))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "linearSVM_confusion_matrix = metrics.confusion_matrix(test_class, predicted_svm)\n",
    "print(linearSVM_confusion_matrix)\n",
    "\n",
    "predicted_svm_train = text_clf_svm.predict(train_data)\n",
    "print('Train Accuracy:'+str(np.mean(predicted_svm_train == train_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.601265822785\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.57      0.71      0.63       407\n",
      "          0       0.58      0.50      0.53       373\n",
      "          1       0.68      0.59      0.63       326\n",
      "\n",
      "avg / total       0.61      0.60      0.60      1106\n",
      "\n",
      "Confusion Matrix:\n",
      "[[288  84  35]\n",
      " [132 185  56]\n",
      " [ 83  51 192]]\n",
      "Train Accuracy:0.823807380738\n"
     ]
    }
   ],
   "source": [
    "clf1 = MultinomialNB(alpha= 0.094, fit_prior= True)\n",
    "clf2 = SGDClassifier(alpha=0.001,learning_rate='optimal',loss= 'epsilon_insensitive', penalty= 'l2',n_iter = 100, random_state=42)\n",
    "clf3 = LinearSVC(C = 0.5, loss = 'hinge', random_state= 42)\n",
    "clf4 = RandomForestClassifier(n_estimators = 22, class_weight = 'balanced_subsample', random_state = 42,criterion=\"gini\")\n",
    "\n",
    "eclf = Pipeline([('vect', CountVectorizer(max_features=4800,ngram_range=(1,2))),\n",
    "                    ('tfidf', TfidfTransformer(use_idf= True)),\n",
    "                    ('clf', VotingClassifier(estimators=[('mnb', clf1), ('sgd', clf2), ('svm', clf3), ('rf',clf4)], voting='hard'))])\n",
    "\n",
    "eclf = eclf.fit(train_data,train_label)\n",
    "\n",
    "p = eclf.predict(test_data)\n",
    "print('Test Accuracy:'+str(np.mean(p==test_class)))\n",
    "print(metrics.classification_report(test_class, p))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "voting_confusion_matrix = metrics.confusion_matrix(test_class,p)\n",
    "print(voting_confusion_matrix)\n",
    "\n",
    "predicted_eclf_train = eclf.predict(train_data)\n",
    "print('Train Accuracy:'+str(np.mean(predicted_eclf_train == train_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other approaces tried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.597649186257\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.57      0.70      0.63       407\n",
      "          0       0.59      0.47      0.52       373\n",
      "          1       0.64      0.62      0.63       326\n",
      "\n",
      "avg / total       0.60      0.60      0.59      1106\n",
      "\n",
      "Confusion Matrix:\n",
      "[[285  78  44]\n",
      " [131 174  68]\n",
      " [ 80  44 202]]\n",
      "Train Accuracy:0.763726372637\n"
     ]
    }
   ],
   "source": [
    "# parameterssgd = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#               'vect__max_features': list(range(100,5000,500)),\n",
    "#               'tfidf__use_idf': (True, False),\n",
    "#               'clf__loss' : ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', \n",
    "#                              'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "#               'clf__penalty' : ['l2', 'l1', 'elasticnet'],\n",
    "#               'clf__learning_rate' : ['optimal','invscaling'],\n",
    "#               'clf__alpha': [i/1000 for i in range(1,10)]\n",
    "#              }\n",
    "# text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "#                     ('tfidf', TfidfTransformer()),\n",
    "#                     ('clf', SGDClassifier(n_iter = 100, eta0 = 0.0001, random_state=42))])\n",
    "# text_clf = GridSearchCV(text_clf,parameterssgd,n_jobs=-1).fit(train_data, train_label)\n",
    "# text_clf.best_params_\n",
    "\n",
    "text_clf_sgd = Pipeline([('vect', CountVectorizer(max_features=5000,ngram_range=(1,2))),\n",
    "                    ('tfidf', TfidfTransformer(use_idf= True)),\n",
    "                    ('clf', SGDClassifier(alpha=0.001,learning_rate='optimal',loss= 'epsilon_insensitive'\n",
    "                                          ,penalty= 'l2',n_iter = 100, random_state=42))]).fit(train_data, train_label)\n",
    "\n",
    "predicted_sgd = text_clf_sgd.predict(test_data)\n",
    "print('Test Accuracy:'+str(np.mean(predicted_sgd == test_class)))\n",
    "print(metrics.classification_report(test_class, predicted_sgd))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "linearSGD_confusion_matrix = metrics.confusion_matrix(test_class, predicted_sgd)\n",
    "print(linearSGD_confusion_matrix)\n",
    "\n",
    "predicted_sgd_train = text_clf_sgd.predict(train_data)\n",
    "print('Train Accuracy:'+str(np.mean(predicted_sgd_train == train_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.553345388788\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.57      0.59      0.58       407\n",
      "          0       0.52      0.57      0.54       373\n",
      "          1       0.58      0.49      0.53       326\n",
      "\n",
      "avg / total       0.56      0.55      0.55      1106\n",
      "\n",
      "Confusion Matrix:\n",
      "[[240 119  48]\n",
      " [ 94 213  66]\n",
      " [ 90  77 159]]\n",
      "Train Accuracy:0.995274527453\n"
     ]
    }
   ],
   "source": [
    "# parametersrf = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#               'vect__max_features': list(range(500,5000,100)),\n",
    "#               'tfidf__use_idf': (True, False),\n",
    "#               'clf__criterion' : ['gini','entropy'],\n",
    "#                 'clf__n_estimators' : list(range(2,25,10)),\n",
    "#                 'clf__class_weight' : ['balanced_subsample']\n",
    "#                 }\n",
    "\n",
    "# text_clf_RandomForest = Pipeline([('vect', CountVectorizer()),\n",
    "#                     ('tfidf', TfidfTransformer()),\n",
    "#                     ('clf', RandomForestClassifier(random_state = 42))])\n",
    "# text_clf_RandomForest = GridSearchCV(text_clf_RandomForest,parametersrf,n_jobs=-1).fit(train_data, train_label)\n",
    "# text_clf_RandomForest.best_params_\n",
    "\n",
    "text_clf_RandomForest = Pipeline([('vect', CountVectorizer(max_features = 4700,ngram_range=(1,2))),\n",
    "                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ('clf', RandomForestClassifier(random_state = 42,criterion=\"gini\",class_weight='balanced_subsample'\n",
    "                                                   ,n_estimators=22))])\n",
    "text_clf_RandomForest = text_clf_RandomForest.fit(train_data, train_label)\n",
    "# {'clf__class_weight': 'balanced_subsample',\n",
    "#  'clf__criterion': 'gini',\n",
    "#  'clf__n_estimators': 22,\n",
    "#  'tfidf__use_idf': True,\n",
    "#  'vect__max_features': 4700,\n",
    "#  'vect__ngram_range': (1, 2)}\n",
    "predicted_rf = text_clf_RandomForest.predict(test_data)\n",
    "print('Test Accuracy:'+str(np.mean(predicted_rf == test_class)))\n",
    "print(metrics.classification_report(test_class, predicted_rf))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "linearRF_confusion_matrix = metrics.confusion_matrix(test_class, predicted_rf)\n",
    "print(linearRF_confusion_matrix)\n",
    "\n",
    "predicted_rf_train = text_clf_RandomForest.predict(train_data)\n",
    "print('Train Accuracy:'+str(np.mean(predicted_rf_train == train_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Data for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combineFrameData = [train_data, test_data]\n",
    "combineFrameLabel = [train_label, test_class]\n",
    "combineTrainDataDF = pd.concat(combineFrameData)\n",
    "combineTrainLabelDF = pd.concat(combineFrameLabel)\n",
    "count_vect_kfold = CountVectorizer(max_features = 4800)\n",
    "X_train_counts_kfold = count_vect.fit_transform(combineTrainDataDF)\n",
    "tf_transformer_kfold = TfidfTransformer(use_idf=True).fit(X_train_counts_kfold)\n",
    "X_train_tf_kfold = tf_transformer_kfold.transform(X_train_counts_kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation accuracy Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58 (+/- 0.04)\n",
      "[ 0.55215827  0.55935252  0.59892086  0.57837838  0.57477477  0.58918919\n",
      "  0.58918919  0.61441441  0.59205776  0.55153707]\n"
     ]
    }
   ],
   "source": [
    "clf_nb_kfold = MultinomialNB(alpha = 0.099, fit_prior = True)\n",
    "scores = cross_val_score(clf_nb_kfold, X_train_tf_kfold, combineTrainLabelDF, cv=10)\n",
    "clf_nb_kfold = clf_nb_kfold.fit(X_train_tf_kfold, combineTrainLabelDF)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation Accuracy Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61 (+/- 0.04)\n",
      "[ 0.59532374  0.59352518  0.62589928  0.61801802  0.60900901  0.61801802\n",
      "  0.62882883  0.62522523  0.58303249  0.57504521]\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_kfold = LinearSVC(C = 0.5, loss = 'hinge', penalty='l2', random_state= 42, max_iter=10000)\n",
    "scores_lsvm = cross_val_score(clf_lsvm_kfold, X_train_tf_kfold, combineTrainLabelDF, cv=10)\n",
    "clf_lsvm_kfold = clf_lsvm_kfold.fit(X_train_tf_kfold, combineTrainLabelDF)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_lsvm.mean(), scores_lsvm.std() * 2))\n",
    "print(scores_lsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.60 (+/- 0.03)\n",
      "[ 0.58992806  0.60431655  0.62410072  0.61261261  0.58738739  0.6036036\n",
      "  0.61441441  0.61981982  0.59566787  0.56600362]\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_kfold = SGDClassifier(alpha=0.001,learning_rate='optimal',loss= 'epsilon_insensitive', penalty= 'l2',n_iter = 100, random_state=42)\n",
    "scores_sgd = cross_val_score(clf_sgd_kfold, X_train_tf_kfold, combineTrainLabelDF, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_sgd.mean(), scores_sgd.std() * 2))\n",
    "print(scores_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation accuracy Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57 (+/- 0.04)\n",
      "[ 0.56115108  0.58633094  0.5647482   0.58918919  0.59279279  0.58558559\n",
      "  0.52972973  0.56216216  0.54693141  0.55334539]\n"
     ]
    }
   ],
   "source": [
    "clf_randomForest_kfold = RandomForestClassifier(n_estimators = 22, class_weight = 'balanced_subsample', random_state = 42, criterion=\"gini\")\n",
    "scores_randomForest = cross_val_score(clf_randomForest_kfold, X_train_tf_kfold, combineTrainLabelDF, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_randomForest.mean(), scores_randomForest.std() * 2))\n",
    "print(scores_randomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "with open('nb_dumped_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(clf, fid)  \n",
    "with open('svm_dumped_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(text_clf_svm, fid) \n",
    "with open('voting_dumped_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(eclf, fid) \n",
    "# # load a classifier\n",
    "# with open('svm_dumped_classifier_romney.pkl', 'rb') as fid:\n",
    "#     gnb_loaded = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weblink obama weblink has to maintain his prof...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weblink obama weblink went into the debat swin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ditto i start weblink weblink year ago weblink...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>i absolut posit weblink obama weblink s view i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>im agre complet with weblink obama weblink s s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>weblink obama weblink s weblink posit weblink ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>hahahahahaahahha weblink obama weblink s webli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>if you think the economi has gotten negat dure...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>weblink obama weblink s weblink debat perform ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>i posit the fact that it not within weblink ob...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>presid weblink obama weblink s weblink polici ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>posit weblink obama weblink s weblink defens o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>weblink obama weblink s weblink haircut weblin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>posit presid weblink obama weblink s extempora...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>look at weblink obama weblink s weblink walk w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>irrroc weblink obama weblink s weblink spend w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>weblink obama weblink s on his posit weblink g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>the debat isnt even over and weblink obama web...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>weblink obama weblink s weblink respons weblin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>posit weblink obama weblink s weblink respons ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>weblink obama weblink s weblink answer weblink...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>jonshuman sure i bet he was my dad now has a p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weblink weblink i posit weblink obama weblink ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>yes weblink obama weblink s weblink main focus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>but i know im weblink vote weblink for weblink...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>weblink barackobama weblink ive got your back ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>i posit weblink obama weblink truth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>i negat when peopl say weblink obama weblink h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>i want my next boyfriend to weblink look webli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>yes weblink obama weblink is an idealist but h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>negat obamareagan pic weblink negat negat nega...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>negat when i hear about a terrorist negat nega...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>negat negat news energis weblink obama weblink...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>negat weblink weblink obama weblink come from ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>negat i posit a weblink video weblink weblink ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>negat what do all these celeb have in common w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>negat is weblink obama weblink head to vega ag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>negat hahaha weblink michell obama weblink sho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>negat check out airhead actress in new proobam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>negat posit i taught us histori student about ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>negat weblink obama weblink masih pake bukanny...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>negat weblink obama weblink make educ a weblin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>negat no hope except through god mike church s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>negat weblink obama weblink campaign posit kno...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>negat weblink obama weblink say when i was web...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>negat weblink and they posit weblink obama web...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>negat negat bobbi jindal he dont do negat for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>negat has weblink obama weblink negat the huff...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>negat obama negat negat negat negat negat nega...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>negat weblink if true negat it might be due to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>negat crowley let weblink obama weblink have l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>negat weblink you do know that obama not in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>negat negat orang broccoli weblink obama webli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>negat supremecourt agre to hear case on federa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>negat negat on phish lotto x weblink email pos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>negat thought there was an invas go on outsid ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>negat weblink anoth christian universityetbu h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>negat negat weblink pm weblink weblink gender ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>negat weblink it leav out other black immigr o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>negat rae sky negat my compani stock hit a web...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1951 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet Class\n",
       "2     weblink obama weblink has to maintain his prof...     1\n",
       "11    weblink obama weblink went into the debat swin...     1\n",
       "21    ditto i start weblink weblink year ago weblink...     1\n",
       "36    i absolut posit weblink obama weblink s view i...     1\n",
       "43    im agre complet with weblink obama weblink s s...     1\n",
       "46    weblink obama weblink s weblink posit weblink ...     1\n",
       "47    hahahahahaahahha weblink obama weblink s webli...     1\n",
       "48    if you think the economi has gotten negat dure...     1\n",
       "50    weblink obama weblink s weblink debat perform ...     1\n",
       "51    i posit the fact that it not within weblink ob...     1\n",
       "56    presid weblink obama weblink s weblink polici ...     1\n",
       "57    posit weblink obama weblink s weblink defens o...     1\n",
       "58    weblink obama weblink s weblink haircut weblin...     1\n",
       "59    posit presid weblink obama weblink s extempora...     1\n",
       "64    look at weblink obama weblink s weblink walk w...     1\n",
       "66    irrroc weblink obama weblink s weblink spend w...     1\n",
       "69    weblink obama weblink s on his posit weblink g...     1\n",
       "71    the debat isnt even over and weblink obama web...     1\n",
       "72    weblink obama weblink s weblink respons weblin...     1\n",
       "75    posit weblink obama weblink s weblink respons ...     1\n",
       "76    weblink obama weblink s weblink answer weblink...     1\n",
       "78    jonshuman sure i bet he was my dad now has a p...     1\n",
       "83    weblink weblink i posit weblink obama weblink ...     1\n",
       "84    yes weblink obama weblink s weblink main focus...     1\n",
       "90    but i know im weblink vote weblink for weblink...     1\n",
       "96    weblink barackobama weblink ive got your back ...     1\n",
       "100                 i posit weblink obama weblink truth     1\n",
       "103   i negat when peopl say weblink obama weblink h...     1\n",
       "109   i want my next boyfriend to weblink look webli...     1\n",
       "113   yes weblink obama weblink is an idealist but h...     1\n",
       "...                                                 ...   ...\n",
       "2319  negat obamareagan pic weblink negat negat nega...     0\n",
       "2320  negat when i hear about a terrorist negat nega...     0\n",
       "2321  negat negat news energis weblink obama weblink...     0\n",
       "2323  negat weblink weblink obama weblink come from ...     0\n",
       "2327  negat i posit a weblink video weblink weblink ...     0\n",
       "2335  negat what do all these celeb have in common w...     0\n",
       "2336  negat is weblink obama weblink head to vega ag...     0\n",
       "2338  negat hahaha weblink michell obama weblink sho...     0\n",
       "2340  negat check out airhead actress in new proobam...     0\n",
       "2341  negat posit i taught us histori student about ...     0\n",
       "2342  negat weblink obama weblink masih pake bukanny...     0\n",
       "2347  negat weblink obama weblink make educ a weblin...     0\n",
       "2348  negat no hope except through god mike church s...     0\n",
       "2350  negat weblink obama weblink campaign posit kno...     0\n",
       "2351  negat weblink obama weblink say when i was web...     0\n",
       "2353  negat weblink and they posit weblink obama web...     0\n",
       "2355  negat negat bobbi jindal he dont do negat for ...     0\n",
       "2358  negat has weblink obama weblink negat the huff...     0\n",
       "2359  negat obama negat negat negat negat negat nega...     0\n",
       "2360  negat weblink if true negat it might be due to...     0\n",
       "2365  negat crowley let weblink obama weblink have l...     0\n",
       "2366  negat weblink you do know that obama not in th...     0\n",
       "2367  negat negat orang broccoli weblink obama webli...     0\n",
       "2373  negat supremecourt agre to hear case on federa...     0\n",
       "2377  negat negat on phish lotto x weblink email pos...     0\n",
       "2380  negat thought there was an invas go on outsid ...     0\n",
       "2381  negat weblink anoth christian universityetbu h...     0\n",
       "2392  negat negat weblink pm weblink weblink gender ...     0\n",
       "2397  negat weblink it leav out other black immigr o...     0\n",
       "2400  negat rae sky negat my compani stock hit a web...     0\n",
       "\n",
       "[1951 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_excel('testing-Obama-Romney-tweets.xlsx', sheetname = 'Obama')#, header=None)\n",
    "df_test.columns = ['tweet','Class']\n",
    "df_test['Class'] = df_test['Class'].astype(str) \n",
    "inputFrame1 = df_test[df_test.Class == '1']\n",
    "inputFrame2 = df_test[df_test.Class == '-1']\n",
    "inputFrame3 = df_test[df_test.Class == '0']\n",
    "df_test = pd.concat([inputFrame1, inputFrame2, inputFrame3])\n",
    "df_test['tweet'] = df_test['tweet'].apply(processTweet)\n",
    "df_test['tweet'] = df_test['tweet'].apply(joinList1)\n",
    "\n",
    "testcounts = count_vect.transform(df_test['tweet'])\n",
    "test_tfidf = tf_transformer_kfold.transform(testcounts)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.556637621732\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.55      0.60      0.58       688\n",
      "          0       0.53      0.54      0.53       681\n",
      "          1       0.60      0.53      0.56       582\n",
      "\n",
      "avg / total       0.56      0.56      0.56      1951\n",
      "\n",
      "Confusion Matrix:\n",
      "[[411 187  90]\n",
      " [199 368 114]\n",
      " [131 144 307]]\n"
     ]
    }
   ],
   "source": [
    "p = clf_nb_kfold.predict(test_tfidf)\n",
    "xy = pd.to_numeric(df_test['Class'])\n",
    "print('Test Accuracy:'+str(np.mean(p == xy)))\n",
    "\n",
    "print(metrics.classification_report(xy, p))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "nb_confusion_matrix = metrics.confusion_matrix(xy, p)\n",
    "print(nb_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.59354177345\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.58      0.67      0.62       688\n",
      "          0       0.58      0.56      0.57       681\n",
      "          1       0.62      0.55      0.58       582\n",
      "\n",
      "avg / total       0.59      0.59      0.59      1951\n",
      "\n",
      "Confusion Matrix:\n",
      "[[460 139  89]\n",
      " [196 380 105]\n",
      " [133 131 318]]\n"
     ]
    }
   ],
   "source": [
    "p = clf_lsvm_kfold.predict(test_tfidf)\n",
    "xy = pd.to_numeric(df_test['Class'])\n",
    "print('Test Accuracy:'+str(np.mean(p == xy)))\n",
    "\n",
    "print(metrics.classification_report(xy, p))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "nb_confusion_matrix = metrics.confusion_matrix(xy, p)\n",
    "print(nb_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.581752947207\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.56      0.68      0.62       688\n",
      "          0       0.58      0.54      0.56       681\n",
      "          1       0.62      0.51      0.56       582\n",
      "\n",
      "avg / total       0.59      0.58      0.58      1951\n",
      "\n",
      "Confusion Matrix:\n",
      "[[471 135  82]\n",
      " [218 365  98]\n",
      " [149 134 299]]\n"
     ]
    }
   ],
   "source": [
    "p = eclf.predict(df_test['tweet'])\n",
    "xy = pd.to_numeric(df_test['Class'])\n",
    "print('Test Accuracy:'+str(np.mean(p == xy)))\n",
    "\n",
    "print(metrics.classification_report(xy, p))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "nb_confusion_matrix = metrics.confusion_matrix(xy, p)\n",
    "print(nb_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
